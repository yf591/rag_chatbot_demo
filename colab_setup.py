#!/usr/bin/env python3
"""
Google Colabç’°å¢ƒã§RAGãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä½¿ç”¨æ–¹æ³•
1. Google Colab Proã§GPUï¼ˆT4/L4/A100ï¼‰ã‚’æœ‰åŠ¹åŒ–
2. ç’°å¢ƒå¤‰æ•°ï¼ˆHUGGINGFACE_TOKEN, NGROK_TOKENï¼‰ã‚’è¨­å®š
3. ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ
"""

import os
import sys
import subprocess
import torch
from pathlib import Path

def check_gpu():
    """GPUåˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
    print("=== GPUæƒ…å ± ===")
    if torch.cuda.is_available():
        print(f"âœ… CUDAåˆ©ç”¨å¯èƒ½")
        print(f"GPUæ•°: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
            print(f"ãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f}GB")
    else:
        print("âŒ CUDAåˆ©ç”¨ä¸å¯ - CPUãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã•ã‚Œã¾ã™")
    print()

def check_environment():
    """ç’°å¢ƒå¤‰æ•°ã‚’ãƒã‚§ãƒƒã‚¯"""
    print("=== ç’°å¢ƒå¤‰æ•°ãƒã‚§ãƒƒã‚¯ ===")
    required_vars = ["HUGGINGFACE_TOKEN", "NGROK_TOKEN"]
    
    for var in required_vars:
        value = os.getenv(var)
        if value:
            print(f"âœ… {var}: è¨­å®šæ¸ˆã¿")
        else:
            print(f"âŒ {var}: æœªè¨­å®š")
    print()

def check_files():
    """å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª"""
    print("=== ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª ===")
    required_files = [
        "app.py",
        "rag_system.py", 
        "vector_store/index.faiss",
        "vector_store/documents.pkl",
        ".env"
    ]
    
    for file_path in required_files:
        if Path(file_path).exists():
            print(f"âœ… {file_path}: å­˜åœ¨")
        else:
            print(f"âŒ {file_path}: ä¸è¶³")
    print()

def setup_streamlit_secrets():
    """Streamlitç”¨ã®secrets.tomlãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¨­å®š"""
    print("=== Streamlitè¨­å®š ===")
    
    # .streamlitãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    streamlit_dir = Path(".streamlit")
    streamlit_dir.mkdir(exist_ok=True)
    
    # secrets.tomlãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    secrets_content = f"""[huggingface]
token = "{os.getenv('HUGGINGFACE_TOKEN', '')}"

[ngrok]
token = "{os.getenv('NGROK_TOKEN', '')}"
"""
    
    secrets_path = streamlit_dir / "secrets.toml"
    with open(secrets_path, "w") as f:
        f.write(secrets_content)
    
    print(f"âœ… {secrets_path}: ä½œæˆå®Œäº†")
    print()

def install_dependencies():
    """ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«"""
    print("=== ä¾å­˜é–¢ä¿‚ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« ===")
    try:
        subprocess.run([sys.executable, "-m", "pip", "install", "-r", "requirements.txt"], 
                      check=True, capture_output=True, text=True)
        print("âœ… ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†")
    except subprocess.CalledProcessError as e:
        print(f"âŒ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
        print(f"å‡ºåŠ›: {e.stdout}")
        print(f"ã‚¨ãƒ©ãƒ¼: {e.stderr}")
    print()

def verify_model_access():
    """ãƒ¢ãƒ‡ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ç¢ºèª"""
    print("=== ãƒ¢ãƒ‡ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ç¢ºèª ===")
    try:
        from huggingface_hub import login, whoami
        
        token = os.getenv("HUGGINGFACE_TOKEN")
        if token:
            login(token=token)
            user_info = whoami()
            print(f"âœ… HuggingFaceèªè¨¼æˆåŠŸ: {user_info['name']}")
        else:
            print("âŒ HuggingFaceãƒˆãƒ¼ã‚¯ãƒ³ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
    except Exception as e:
        print(f"âŒ èªè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
    print()

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸš€ Google Colab RAGãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—")
    print("=" * 50)
    
    check_gpu()
    check_environment()
    check_files()
    setup_streamlit_secrets()
    install_dependencies()
    verify_model_access()
    
    print("=" * 50)
    print("âœ… ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†!")
    print()
    print("æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("1. 'streamlit run app.py' ã‚’å®Ÿè¡Œ")
    print("2. ngrokã§ãƒˆãƒ³ãƒãƒ«ã‚’ä½œæˆ")
    print("3. å…¬é–‹URLã«ã‚¢ã‚¯ã‚»ã‚¹")

if __name__ == "__main__":
    main()
