import streamlit as st
import os
import torch
from rag_system import RAGSystem

# Streamlitãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ç¦å±±å¸‚RAGãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ",
    page_icon="ğŸ›ï¸",
    layout="wide"
)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if 'rag_system' not in st.session_state:
    st.session_state.rag_system = None
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []

def initialize_rag_system():
    """RAGã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–"""
    if st.session_state.rag_system is None:
        with st.spinner("RAGã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ä¸­...ï¼ˆåˆå›ã¯2-3åˆ†ã‹ã‹ã‚Šã¾ã™ï¼‰"):
            try:
                # Streamlit secretsã‹ã‚‰HuggingFaceãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—
                hf_token = st.secrets.get("huggingface", {}).get("token", None)
                if not hf_token:
                    # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ã‚‚è©¦ã™
                    hf_token = os.getenv("HUGGINGFACE_TOKEN")
                
                if not hf_token:
                    st.error("HuggingFaceãƒˆãƒ¼ã‚¯ãƒ³ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
                    return False
                
                st.session_state.rag_system = RAGSystem(hf_token=hf_token)
                st.success("RAGã‚·ã‚¹ãƒ†ãƒ ãŒæ­£å¸¸ã«åˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸï¼")
                
                # GPUä½¿ç”¨çŠ¶æ³ã‚’è¡¨ç¤º
                if torch.cuda.is_available():
                    gpu_name = torch.cuda.get_device_name(0)
                    st.info(f"ğŸš€ GPUä½¿ç”¨ä¸­: {gpu_name}")
                else:
                    st.warning("âš ï¸ CPUãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œä¸­")
                
            except Exception as e:
                st.error(f"RAGã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                return False
    return True

def main():
    st.title("ğŸ›ï¸ ç¦å±±å¸‚RAGãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ")
    st.markdown("ç¦å±±å¸‚ã®å…¬å¼è³‡æ–™ã«åŸºã¥ã„ã¦è³ªå•ã«ãŠç­”ãˆã—ã¾ã™ã€‚")
    
    # GPUæƒ…å ±è¡¨ç¤º
    if torch.cuda.is_available():
        gpu_info = f"GPU: {torch.cuda.get_device_name(0)} | ãƒ¡ãƒ¢ãƒª: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB"
        st.sidebar.success(f"ğŸš€ {gpu_info}")
    else:
        st.sidebar.warning("âš ï¸ CPUãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œä¸­")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        st.header("ğŸ“‹ ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±")
        st.markdown("""
        **åˆ©ç”¨å¯èƒ½ãªè³‡æ–™:**
        - ç¬¬å››æ¬¡ç¦å±±å¸‚ç·åˆè¨ˆç”»
        - ç¦å±±ã¿ã‚‰ã„å‰µé€ ãƒ“ã‚¸ãƒ§ãƒ³
        - ç¦å±±å¸‚è¦³å…‰ãƒ‘ãƒ³ãƒ•ãƒ¬ãƒƒãƒˆ
        
        **ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«:**
        - LLM: tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.1
        - åŸ‹ã‚è¾¼ã¿: all-MiniLM-L6-v2
        - é‡å­åŒ–: 4bit QLoRA
        """)
        
        if st.button("ğŸ”„ ã‚·ã‚¹ãƒ†ãƒ å†åˆæœŸåŒ–"):
            st.session_state.rag_system = None
            st.session_state.chat_history = []
            st.rerun()
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡è¡¨ç¤º
        if torch.cuda.is_available():
            memory_used = torch.cuda.memory_allocated(0) / 1e9
            memory_total = torch.cuda.get_device_properties(0).total_memory / 1e9
            st.metric("GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡", f"{memory_used:.1f}GB / {memory_total:.1f}GB")
    
    # RAGã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    if not initialize_rag_system():
        st.stop()
    
    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´è¡¨ç¤º
    if st.session_state.chat_history:
        st.subheader("ğŸ’¬ ãƒãƒ£ãƒƒãƒˆå±¥æ­´")
        for i, (question, answer, sources) in enumerate(st.session_state.chat_history):
            with st.expander(f"Q{i+1}: {question[:50]}..."):
                st.markdown(f"**è³ªå•:** {question}")
                st.markdown(f"**å›ç­”:** {answer}")
                if sources:
                    st.markdown(f"**å‚è€ƒè³‡æ–™:** {', '.join(sources)}")
    
    # è³ªå•å…¥åŠ›
    st.subheader("â“ è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
    
    # è³ªå•ä¾‹
    st.markdown("**è³ªå•ä¾‹:**")
    example_questions = [
        "ç¦å±±å¸‚ã®äººå£ã¯ã©ã®ãã‚‰ã„ã§ã™ã‹ï¼Ÿ",
        "ç¦å±±å¸‚ã®ä¸»è¦ãªè¦³å…‰ã‚¹ãƒãƒƒãƒˆã‚’æ•™ãˆã¦ãã ã•ã„",
        "ç¦å±±å¸‚ã®å°†æ¥ãƒ“ã‚¸ãƒ§ãƒ³ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
        "ç¦å±±å¸‚ã®ç”£æ¥­ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„"
    ]
    
    cols = st.columns(2)
    for i, example in enumerate(example_questions):
        col = cols[i % 2]
        if col.button(example, key=f"example_{i}"):
            st.session_state.current_question = example
    
    # è³ªå•å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
    with st.form("question_form"):
        question = st.text_area(
            "è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:",
            value=st.session_state.get('current_question', ''),
            height=100,
            placeholder="ä¾‹: ç¦å±±å¸‚ã®è¦³å…‰ã‚¹ãƒãƒƒãƒˆã‚’æ•™ãˆã¦ãã ã•ã„"
        )
        submitted = st.form_submit_button("ğŸ’¬ è³ªå•ã™ã‚‹")
        
        if submitted and question.strip():
            with st.spinner("LLMã§å›ç­”ã‚’ç”Ÿæˆä¸­..."):
                try:
                    # RAGã‚·ã‚¹ãƒ†ãƒ ã§å›ç­”ç”Ÿæˆ
                    result = st.session_state.rag_system.answer_question(question)
                    
                    # çµæœè¡¨ç¤º
                    st.subheader("ğŸ¤– å›ç­”")
                    st.markdown(result['answer'])
                    
                    if result['sources']:
                        st.subheader("ğŸ“š å‚è€ƒè³‡æ–™")
                        for source in result['sources']:
                            st.markdown(f"- {source}")
                    
                    # è©³ç´°æƒ…å ±ã®è¡¨ç¤º
                    if result['retrieved_docs']:
                        with st.expander("ğŸ“„ æ¤œç´¢ã•ã‚ŒãŸæ–‡æ›¸ã®è©³ç´°"):
                            for i, doc in enumerate(result['retrieved_docs']):
                                st.markdown(f"**æ–‡æ›¸ {i+1}:** {doc['metadata']['source']}")
                                st.markdown(f"```\n{doc['content'][:500]}...\n```")
                    
                    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã«è¿½åŠ 
                    st.session_state.chat_history.append((
                        question,
                        result['answer'],
                        result['sources']
                    ))
                    
                    # ç¾åœ¨ã®è³ªå•ã‚’ã‚¯ãƒªã‚¢
                    if 'current_question' in st.session_state:
                        del st.session_state.current_question
                    
                except Exception as e:
                    st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    
    # ãƒ•ãƒƒã‚¿ãƒ¼
    st.markdown("---")
    st.markdown("""
    <div style='text-align: center; color: gray;'>
    ç¦å±±å¸‚RAGãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ - GPUç‰ˆ - å…¬å¼è³‡æ–™ã«åŸºã¥ãæƒ…å ±æä¾›ã‚·ã‚¹ãƒ†ãƒ 
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
