# RAGç²¾åº¦æ”¹å–„ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—

## ğŸ¯ **ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¦‚è¦**

æœ¬ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€Œrag-chatbot-demoã€ã¯ã€è‡ªæ²»ä½“å‘ã‘RAGï¼ˆRetrieval-Augmented Generationï¼‰ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®**ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—**ã§ã™ã€‚

### ãƒ‡ãƒ¢ç‰ˆã¨ã—ã¦ã®ä½ç½®ã¥ã‘
- **ç›®çš„**: RAGã‚·ã‚¹ãƒ†ãƒ ã®æ¦‚å¿µå®Ÿè¨¼ã¨æŠ€è¡“çš„å¯èƒ½æ€§ã®æç¤º
- **ç¾çŠ¶**: åŸºæœ¬çš„ãªæ©Ÿèƒ½å®Ÿè£…ã¨Google Colabå¯¾å¿œã‚’å„ªå…ˆ
- **å°†æ¥**: ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç’°å¢ƒã¸ã®ç™ºå±•å¯èƒ½æ€§ã‚’è€ƒæ…®ã—ãŸæ”¹å–„æ¡ˆã‚’æç¤º

---

## ğŸ“Š **ç¾åœ¨ã®å®Ÿè£…çŠ¶æ³åˆ†æ**

### è»½é‡ç‰ˆå®Ÿè£… (`pdf_processor_light.py`)
```python
class PDFProcessor:
    def __init__(self, pdf_directory: str = "./"):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,  # ğŸ” æ”¹å–„ç‚¹1: å°ã•ã™ãã‚‹ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
            chunk_overlap=100,
            length_function=len,
        )
        self.embeddings = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",  # ğŸ” æ”¹å–„ç‚¹2: è»½é‡ãƒ¢ãƒ‡ãƒ«
            model_kwargs={'device': 'cpu'}
        )
```

### æ¨™æº–ç‰ˆå®Ÿè£… (`pdf_processor.py`)
```python
class PDFProcessor:
    def __init__(self, pdf_directory: str = "./"):
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,  # âœ… æ”¹å–„æ¸ˆã¿: ã‚ˆã‚Šé©åˆ‡ãªã‚µã‚¤ã‚º
            chunk_overlap=200,
            length_function=len,
        )
        self.embeddings = HuggingFaceEmbeddings(
            model_name="intfloat/multilingual-e5-large",  # âœ… æ”¹å–„æ¸ˆã¿: å¤šè¨€èªé«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«
            model_kwargs={'device': 'cpu'}
        )
```

### ç¾åœ¨ã®åˆ¶é™äº‹é …
1. **PDFå‡¦ç†**: è»½é‡ç‰ˆã§ã¯10ãƒšãƒ¼ã‚¸ã®ã¿å‡¦ç†ï¼ˆãƒ‡ãƒ¢ç”¨åˆ¶é™ï¼‰
2. **æ¤œç´¢ç²¾åº¦**: ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®ã¿
3. **ç”Ÿæˆå“è³ª**: åŸºæœ¬çš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã¿
4. **è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ **: å®šé‡çš„ãªç²¾åº¦æ¸¬å®šãªã—

---

## ğŸš€ **æ®µéšåˆ¥æ”¹å–„ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—**

## Phase 1: å³åº§ã«å®Ÿè£…å¯èƒ½ãªæ”¹å–„ï¼ˆCPUç’°å¢ƒå¯¾å¿œï¼‰

### 1.1 PDFå‡¦ç†ã®å¼·åŒ–

#### **ãƒ•ã‚¡ã‚¤ãƒ«**: `pdf_processor_light.py` â†’ `pdf_processor_enhanced.py`

ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯æ—¢å­˜ã®è»½é‡ç‰ˆPDFå‡¦ç†å™¨ã‚’æ”¹è‰¯ã—ã€ã‚ˆã‚Šé«˜åº¦ãªPDFå‡¦ç†æ©Ÿèƒ½ã‚’æŒã¤ã‚ˆã†ã«ä¿®æ­£ã‚’åŠ ãˆãŸã‚‚ã®ã‚’æƒ³å®šã—ã¦ã„ã¾ã™ã€‚å…¨ãƒšãƒ¼ã‚¸å‡¦ç†ã€ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ã®å¼·åŒ–ã€ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å……å®Ÿã«ã‚ˆã‚Šã€æ¤œç´¢ç²¾åº¦ã®å‘ä¸Šã‚’ç›®æŒ‡ã—ã¾ã™ã€‚

**ç¾åœ¨ã®ã‚³ãƒ¼ãƒ‰**

ç¾åœ¨ã®å®Ÿè£…ã§ã¯ä»¥ä¸‹ã®èª²é¡ŒãŒã‚ã‚Šã¾ã™ã€‚
- æœ€åˆã®10ãƒšãƒ¼ã‚¸ã®ã¿å‡¦ç†ï¼ˆãƒ‡ãƒ¢ç”¨åˆ¶é™ï¼‰
- åŸºæœ¬çš„ãªãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã®ã¿ã§ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³
- ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†ãŒä¸ååˆ†ã§ãƒã‚¤ã‚ºãŒæ··å…¥

```python
def extract_text_from_pdf(self, pdf_path: str) -> str:
    doc = fitz.open(pdf_path)
    text = ""
    # æœ€åˆã®10ãƒšãƒ¼ã‚¸ã®ã¿å‡¦ç†ï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰
    max_pages = min(10, len(doc))
    for page_num in range(max_pages):
        page = doc[page_num]        text += page.get_text()
```

**æ”¹å–„å¾Œã®ã‚³ãƒ¼ãƒ‰**

ã“ã®æ”¹å–„ã«ã‚ˆã‚Šä»¥ä¸‹ã®åŠ¹æœãŒæœŸå¾…ã§ãã¾ã™ã€‚
- å…¨ãƒšãƒ¼ã‚¸å‡¦ç†ã«ã‚ˆã‚‹æƒ…å ±æ¼ã‚Œã®é˜²æ­¢
- ãƒšãƒ¼ã‚¸ã”ã¨ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¿½åŠ ã«ã‚ˆã‚‹æ¤œç´¢ç²¾åº¦å‘ä¸Š
- ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†ã«ã‚ˆã‚‹ãƒã‚¤ã‚ºé™¤å»ã¨å“è³ªå‘ä¸Š
- ç¦å±±å¸‚å›ºæœ‰ã®ãƒã‚¤ã‚ºãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾å¿œã—ãŸæ­£è¦åŒ–

```python
def extract_text_from_pdf(self, pdf_path: str) -> List[Dict]:
    """PDFã‹ã‚‰ãƒšãƒ¼ã‚¸ã”ã¨ã«ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
    doc = fitz.open(pdf_path)
    pages_data = []
    
    for page_num in range(len(doc)):  # å…¨ãƒšãƒ¼ã‚¸å‡¦ç†
        page = doc[page_num]
        text = page.get_text()
        
        # ãƒ†ã‚­ã‚¹ãƒˆå‰å‡¦ç†
        text = self._preprocess_text(text)
        
        pages_data.append({
            'text': text,
            'page_number': page_num + 1,
            'source_file': os.path.basename(pdf_path),
            'total_pages': len(doc)
        })
    
    doc.close()
    return pages_data

def _preprocess_text(self, text: str) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆã®å‰å‡¦ç†ã¨æ­£è¦åŒ–"""
    import re
    
    # ä¸è¦ãªæ”¹è¡Œãƒ»ç©ºç™½ã®æ­£è¦åŒ–
    text = re.sub(r'\n+', '\n', text)
    text = re.sub(r'\s+', ' ', text)
    
    # æ–‡æ›¸ç‰¹æœ‰ã®ãƒã‚¤ã‚ºé™¤å»
    text = re.sub(r'ãƒšãƒ¼ã‚¸\s*\d+', '', text)  # ãƒšãƒ¼ã‚¸ç•ªå·é™¤å»
    text = re.sub(r'ç¦å±±å¸‚\s*\d{4}å¹´', '', text)  # å¹´åº¦è¡¨è¨˜ã®çµ±ä¸€
    
    return text.strip()
```

### 1.2 ãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥ã®æœ€é©åŒ–

#### **ãƒ•ã‚¡ã‚¤ãƒ«**: `pdf_processor_enhanced.py`

ã“ã®ã‚¯ãƒ©ã‚¹ã§ã¯ã€å¾“æ¥ã®å˜ç´”ãªãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²ã‹ã‚‰ã€æ„å‘³çš„ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä¿æŒã™ã‚‹é«˜åº¦ãªãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥ã‚’å®Ÿè£…ã—ã¾ã™ã€‚æ—¥æœ¬èªç‰¹æœ‰ã®æ–‡æ›¸æ§‹é€ ã«é…æ…®ã—ãŸåˆ†å‰²ã¨ã€ãƒãƒ£ãƒ³ã‚¯å“è³ªã®è©•ä¾¡æ©Ÿèƒ½ã«ã‚ˆã‚Šã€æ¤œç´¢æ™‚ã«ã‚ˆã‚Šé–¢é€£æ€§ã®é«˜ã„æ–‡æ›¸ç‰‡ã‚’å–å¾—ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã¨è€ƒãˆã¦ã„ã¾ã™ã€‚

```python
class EnhancedPDFProcessor:
    def __init__(self, pdf_directory: str = "./"):
        # æ„å‘³çš„åˆ†å‰²ã‚’è€ƒæ…®ã—ãŸã‚¹ãƒ—ãƒªãƒƒã‚¿ãƒ¼
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,           # æœ€é©åŒ–ã•ã‚ŒãŸã‚µã‚¤ã‚º
            chunk_overlap=150,        # æ–‡è„ˆä¿æŒã®ãŸã‚ã®é‡è¤‡
            length_function=len,
            separators=[              # æ—¥æœ¬èªã«æœ€é©åŒ–ã•ã‚ŒãŸåŒºåˆ‡ã‚Šæ–‡å­—
                "\n\n",              # æ®µè½åŒºåˆ‡ã‚Š
                "\n",                # è¡ŒåŒºåˆ‡ã‚Š
                "ã€‚",                 # å¥ç‚¹
                "ã€",                 # èª­ç‚¹
                " ",                 # ã‚¹ãƒšãƒ¼ã‚¹
                ""                   # æ–‡å­—å˜ä½
            ]
        )
        
    def create_semantic_chunks(self, text: str, metadata: Dict) -> List[Dict]:
        """æ„å‘³çš„ãªãƒãƒ£ãƒ³ã‚¯åˆ†å‰²"""
        chunks = self.text_splitter.split_text(text)
        enhanced_chunks = []
        
        for i, chunk in enumerate(chunks):
            # ãƒãƒ£ãƒ³ã‚¯ã®å“è³ªè©•ä¾¡
            if len(chunk.strip()) < 50:  # çŸ­ã™ãã‚‹ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—
                continue
                
            enhanced_chunk = {
                'content': chunk,
                'metadata': {
                    **metadata,
                    'chunk_id': i,
                    'chunk_length': len(chunk),
                    'chunk_quality_score': self._calculate_chunk_quality(chunk)
                }
            }
            enhanced_chunks.append(enhanced_chunk)
            
        return enhanced_chunks
        
    def _calculate_chunk_quality(self, chunk: str) -> float:
        """ãƒãƒ£ãƒ³ã‚¯ã®å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—"""
        # åŸºæœ¬çš„ãªå“è³ªæŒ‡æ¨™
        score = 0.0
        
        # é•·ã•ã‚¹ã‚³ã‚¢ï¼ˆé©åˆ‡ãªé•·ã•ã«é«˜å¾—ç‚¹ï¼‰
        length_score = min(len(chunk) / 500, 1.0)
        
        # æ„å‘³æ€§ã‚¹ã‚³ã‚¢ï¼ˆå¥èª­ç‚¹ã®å­˜åœ¨ï¼‰
        semantic_score = (chunk.count('ã€‚') + chunk.count('ã€')) / max(len(chunk) / 100, 1)
        
        # æƒ…å ±å¯†åº¦ã‚¹ã‚³ã‚¢ï¼ˆæ•°å­—ãƒ»å›ºæœ‰åè©ã®å­˜åœ¨ï¼‰
        import re
        info_density = len(re.findall(r'[0-9ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡]+|ç¦å±±|å¸‚', chunk)) / max(len(chunk) / 50, 1)
        
        score = (length_score + semantic_score + info_density) / 3
        return min(score, 1.0)
```

### 1.3 åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®æ®µéšçš„æ”¹å–„

#### **ãƒ•ã‚¡ã‚¤ãƒ«**: `embedding_manager.py` (æ–°è¦ä½œæˆ)

ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–°è¦ä½œæˆã™ã‚‹ã“ã¨ã§ã€ä»¥ä¸‹ã®åŠ¹æœãŒæœŸå¾…ã§ãã¾ã™ã€‚
- åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®ä¸€å…ƒç®¡ç†ã¨æ€§èƒ½ç‰¹æ€§ã®æ˜ç¢ºåŒ–
- ç”¨é€”ã«å¿œã˜ãŸæœ€é©ãªãƒ¢ãƒ‡ãƒ«é¸æŠã®è‡ªå‹•åŒ–
- ãƒ¢ãƒ‡ãƒ«å¤‰æ›´æ™‚ã®å½±éŸ¿ç¯„å›²ã®å±€æ‰€åŒ–
- å°†æ¥çš„ãªãƒ¢ãƒ‡ãƒ«æ‹¡å¼µã¸ã®æŸ”è»Ÿãªå¯¾å¿œ

```python
class EmbeddingManager:
    """åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®ç®¡ç†ã¨æœ€é©åŒ–"""
    
    MODELS = {
        'light': {
            'name': 'sentence-transformers/all-MiniLM-L6-v2',
            'size': '80MB',
            'performance': 'Low',
            'language': 'Multilingual'
        },
        'standard': {
            'name': 'intfloat/multilingual-e5-large', 
            'size': '1.2GB',
            'performance': 'High',
            'language': 'Multilingual'
        },
        'japanese': {
            'name': 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',
            'size': '420MB', 
            'performance': 'Medium-High',
            'language': 'Japanese-optimized'
        }
    }
    
    def __init__(self, model_type: str = 'standard', device: str = 'cpu'):
        self.model_config = self.MODELS[model_type]
        self.embeddings = HuggingFaceEmbeddings(
            model_name=self.model_config['name'],
            model_kwargs={'device': device}
        )
        
    def get_model_info(self) -> Dict:
        """ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’å–å¾—"""
        return {
            'model_name': self.model_config['name'],
            'estimated_size': self.model_config['size'],
            'expected_performance': self.model_config['performance'],
            'language_support': self.model_config['language']
        }
```

---

## Phase 2: æ¤œç´¢ç²¾åº¦ã®å‘ä¸Š

### 2.1 ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã®å®Ÿè£…

#### **ãƒ•ã‚¡ã‚¤ãƒ«**: `hybrid_retriever.py` (æ–°è¦ä½œæˆ)

ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–°è¦ä½œæˆã™ã‚‹ã“ã¨ã§ã€ä»¥ä¸‹ã®åŠ¹æœãŒæœŸå¾…ã§ãã¾ã™ã€‚
- Denseæ¤œç´¢ã¨Sparseæ¤œç´¢ã®é•·æ‰€ã‚’çµ„ã¿åˆã‚ã›ãŸé«˜ç²¾åº¦æ¤œç´¢
- èªå½™çš„ãƒãƒƒãƒãƒ³ã‚°ï¼ˆBM25ï¼‰ã¨æ„å‘³çš„ãƒãƒƒãƒãƒ³ã‚°ï¼ˆãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã®ç›¸è£œçš„æ´»ç”¨
- æ¤œç´¢çµæœã®å¤šæ§˜æ€§å‘ä¸Šã¨æ¤œç´¢æ¼ã‚Œã®å‰Šæ¸›
- ç¦å±±å¸‚å›ºæœ‰ã®ç”¨èªã«å¯¾ã™ã‚‹é‡ã¿ä»˜ã‘æ©Ÿèƒ½

```python
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever

class HybridRetriever:
    """Denseï¼ˆãƒ™ã‚¯ãƒˆãƒ«ï¼‰+ Sparseï¼ˆBM25ï¼‰ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢"""
    
    def __init__(self, vector_store, documents: List[Dict]):
        # Dense retriever (ç¾åœ¨ã®FAISS)
        self.dense_retriever = vector_store.as_retriever(
            search_kwargs={"k": 10}
        )
        
        # Sparse retriever (BM25)
        texts = [doc['content'] for doc in documents]
        self.sparse_retriever = BM25Retriever.from_texts(
            texts, k=10
        )
        
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¤œç´¢å™¨
        self.ensemble_retriever = EnsembleRetriever(
            retrievers=[self.dense_retriever, self.sparse_retriever],
            weights=[0.7, 0.3]  # Dense:Sparse = 7:3
        )
        
    def retrieve_documents(self, query: str, k: int = 5) -> List[Dict]:
        """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã«ã‚ˆã‚‹æ–‡æ›¸å–å¾—"""
        # ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¤œç´¢å®Ÿè¡Œ
        docs = self.ensemble_retriever.get_relevant_documents(query)
        
        # é‡è¤‡é™¤å»ã¨å†ãƒ©ãƒ³ã‚­ãƒ³ã‚°
        unique_docs = self._remove_duplicates(docs[:k*2])
        reranked_docs = self._rerank_documents(query, unique_docs)
        
        return reranked_docs[:k]
        
    def _rerank_documents(self, query: str, docs: List) -> List:
        """ç°¡æ˜“ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°"""
        scored_docs = []
        for doc in docs:
            # å˜ç´”ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ã‚¹ã‚³ã‚¢
            content = doc.page_content.lower()
            query_lower = query.lower()
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è‡´åº¦
            keyword_score = sum(1 for word in query_lower.split() if word in content)
            
            # ç¦å±±å¸‚é–¢é€£ç”¨èªã®é‡ã¿ä»˜ã‘
            fukuyama_terms = ['ç¦å±±', 'å¸‚', 'è‡ªæ²»ä½“', 'è¡Œæ”¿', 'å¸‚æ°‘']
            fukuyama_score = sum(2 for term in fukuyama_terms if term in content)
            
            total_score = keyword_score + fukuyama_score
            scored_docs.append((doc, total_score))
            
        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        scored_docs.sort(key=lambda x: x[1], reverse=True)
        return [doc for doc, score in scored_docs]
```

### 2.2 æ¤œç´¢ã‚¯ã‚¨ãƒªæ‹¡å¼µ

#### **ãƒ•ã‚¡ã‚¤ãƒ«**: `query_enhancer.py` (æ–°è¦ä½œæˆ)

ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–°è¦ä½œæˆã™ã‚‹ã“ã¨ã§ã€ä»¥ä¸‹ã®åŠ¹æœãŒæœŸå¾…ã§ãã¾ã™ã€‚
- ç¦å±±å¸‚ç‰¹æœ‰ã®ç”¨èªãƒ»åŒç¾©èªã«ã‚ˆã‚‹æ¤œç´¢æ‹¡å¼µ
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ›–æ˜§ãªè³ªå•ã«å¯¾ã™ã‚‹æ¤œç´¢ç²¾åº¦å‘ä¸Š
- è¡Œæ”¿ç”¨èªã¨å¸‚æ°‘ç”¨èªã®æ©‹æ¸¡ã—æ©Ÿèƒ½
- ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¾å­˜ã®æœ€é©ãªã‚¯ã‚¨ãƒªå¤‰æ›

```python
class QueryEnhancer:
    """æ¤œç´¢ã‚¯ã‚¨ãƒªã®æ‹¡å¼µã¨æœ€é©åŒ–"""
    
    def __init__(self):
        # ç¦å±±å¸‚ç‰¹æœ‰ã®ç”¨èªè¾æ›¸
        self.fukuyama_synonyms = {
            'ç¦å±±': ['ç¦å±±å¸‚', 'ãµãã‚„ã¾'],
            'è¦³å…‰': ['è¦³å…‰åœ°', 'åæ‰€', 'è¦³å…‰ã‚¹ãƒãƒƒãƒˆ', 'è¦‹ã©ã“ã‚'],
            'è¨ˆç”»': ['åŸºæœ¬è¨ˆç”»', 'ç·åˆè¨ˆç”»', 'ãƒ—ãƒ©ãƒ³', 'æ§‹æƒ³'],
            'å¸‚æ°‘': ['ä½æ°‘', 'å¸‚æ°‘ã®çš†æ§˜', 'ä½æ°‘ã®æ–¹'],
            'è¡Œæ”¿': ['å¸‚å½¹æ‰€', 'è‡ªæ²»ä½“', 'ç¦å±±å¸‚å½¹æ‰€']
        }
        
    def expand_query(self, query: str) -> str:
        """ã‚¯ã‚¨ãƒªã‚’æ‹¡å¼µã—ã¦æ¤œç´¢ç²¾åº¦ã‚’å‘ä¸Š"""
        expanded_terms = []
        
        for word in query.split():
            expanded_terms.append(word)
            
            # åŒç¾©èªã®è¿½åŠ 
            if word in self.fukuyama_synonyms:
                expanded_terms.extend(self.fukuyama_synonyms[word])
        
        # é‡è¤‡é™¤å»ã—ã¦çµåˆ
        unique_terms = list(set(expanded_terms))
        return ' '.join(unique_terms)
        
    def optimize_query_for_context(self, query: str) -> str:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å¿œã˜ãŸã‚¯ã‚¨ãƒªæœ€é©åŒ–"""
        # ç¦å±±å¸‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ˜ç¤ºçš„è¿½åŠ 
        if 'ç¦å±±' not in query and 'å¸‚' not in query:
            query = f"ç¦å±±å¸‚ {query}"
            
        return query
```

---

## Phase 3: ç”Ÿæˆå“è³ªã®å‘ä¸Š

### 3.1 ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°

#### **ãƒ•ã‚¡ã‚¤ãƒ«**: `rag_system_enhanced.py`

**ç¾åœ¨ã®ã‚³ãƒ¼ãƒ‰** (`rag_system_simple.py`)

ç¾åœ¨ã®å®Ÿè£…ã§ã¯ä»¥ä¸‹ã®èª²é¡ŒãŒã‚ã‚Šã¾ã™ã€‚
- å˜ä¸€ã®æ±ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã¿ã§è³ªå•ã®æ€§è³ªã‚’è€ƒæ…®ã—ã¦ã„ãªã„
- å›ç­”ã®ä¿¡é ¼åº¦ã‚„æ ¹æ‹ ãŒä¸æ˜ç¢º
- å¸‚æ°‘å‘ã‘ã®åˆ†ã‹ã‚Šã‚„ã™ã•ãŒä¸ååˆ†

```python
def generate_response(self, query: str, retrieved_docs: List[Dict]) -> str:
    context = "\n".join([doc['content'] for doc in retrieved_docs])
    
    prompt = f"""
ä»¥ä¸‹ã®æ–‡æ›¸ã‚’å‚è€ƒã«ã€è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

å‚è€ƒæ–‡æ›¸:
{context}

è³ªå•: {query}

å›ç­”:"""
```

**æ”¹å–„å¾Œã®ã‚³ãƒ¼ãƒ‰**

ã“ã®æ”¹å–„ã«ã‚ˆã‚Šä»¥ä¸‹ã®åŠ¹æœãŒæœŸå¾…ã§ãã¾ã™ã€‚
- è³ªå•ã‚«ãƒ†ã‚´ãƒªï¼ˆè¦³å…‰ãƒ»è¨ˆç”»ãƒ»ä¸€èˆ¬ï¼‰ã«å¿œã˜ãŸæœ€é©åŒ–ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
- å›ç­”ã®ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã¨æ ¹æ‹ è³‡æ–™ã®æ˜ç¤º
- å¸‚æ°‘ã«ã‚‚åˆ†ã‹ã‚Šã‚„ã™ã„ä¸å¯§ãªèª¬æ˜å½¢å¼
- è³‡æ–™åã®æ˜ç¤ºã«ã‚ˆã‚‹é€æ˜æ€§å‘ä¸Š

```python
class EnhancedRAGSystem:
    def __init__(self):
        self.prompt_templates = {
            'default': """ã‚ãªãŸã¯ç¦å±±å¸‚ã®è¡Œæ”¿æƒ…å ±ã«é–¢ã™ã‚‹å°‚é–€ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚

ã€å‚è€ƒè³‡æ–™ã€‘
{context}

ã€è³ªå•ã€‘{query}

ã€å›ç­”æŒ‡é‡ã€‘
1. æä¾›ã•ã‚ŒãŸç¦å±±å¸‚ã®å…¬å¼æ–‡æ›¸ã®å†…å®¹ã®ã¿ã«åŸºã¥ã„ã¦å›ç­”ã—ã¦ãã ã•ã„
2. æ ¹æ‹ ã¨ãªã‚‹è³‡æ–™åã‚’æ˜ç¤ºã—ã¦ãã ã•ã„  
3. ä¸æ˜ãªç‚¹ã¯ã€Œæä¾›ã•ã‚ŒãŸè³‡æ–™ã§ã¯ç¢ºèªã§ãã¾ã›ã‚“ã€ã¨å›ç­”ã—ã¦ãã ã•ã„
4. å¸‚æ°‘ã®æ–¹ã«ã‚‚åˆ†ã‹ã‚Šã‚„ã™ã„è¨€è‘‰ã§èª¬æ˜ã—ã¦ãã ã•ã„

ã€å›ç­”ã€‘:""",
            
            'tourism': """ç¦å±±å¸‚ã®è¦³å…‰æƒ…å ±ã«ã¤ã„ã¦ã€ä»¥ä¸‹ã®è³‡æ–™ã«åŸºã¥ã„ã¦ã”æ¡ˆå†…ã„ãŸã—ã¾ã™ã€‚

ã€è¦³å…‰è³‡æ–™ã€‘
{context}

ã€ãŠå•ã„åˆã‚ã›ã€‘{query}

ã€è¦³å…‰æ¡ˆå†…ã€‘
ç¦å±±å¸‚ã®é­…åŠ›ã‚’ãŠä¼ãˆã™ã‚‹ãŸã‚ã€å…¬å¼ãƒ‘ãƒ³ãƒ•ãƒ¬ãƒƒãƒˆã®æƒ…å ±ã‚’ã‚‚ã¨ã«å›ç­”ã„ãŸã—ã¾ã™ï¼š""",
            
            'planning': """ç¦å±±å¸‚ã®å°†æ¥è¨ˆç”»ãƒ»æ”¿ç­–ã«ã¤ã„ã¦èª¬æ˜ã„ãŸã—ã¾ã™ã€‚

ã€è¨ˆç”»è³‡æ–™ã€‘
{context}

ã€ã”è³ªå•ã€‘{query}

ã€æ”¿ç­–èª¬æ˜ã€‘
ç¦å±±å¸‚ã®å…¬å¼è¨ˆç”»æ›¸ã«åŸºã¥ã„ã¦ã€ä»¥ä¸‹ã®ã‚ˆã†ã«ãŠç­”ãˆã—ã¾ã™ï¼š"""
        }
        
    def select_prompt_template(self, query: str) -> str:
        """è³ªå•å†…å®¹ã«å¿œã˜ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé¸æŠ"""
        if any(word in query for word in ['è¦³å…‰', 'åæ‰€', 'è¦‹ã©ã“ã‚', 'ã‚¹ãƒãƒƒãƒˆ']):
            return self.prompt_templates['tourism']
        elif any(word in query for word in ['è¨ˆç”»', 'æ”¿ç­–', 'å°†æ¥', 'ãƒ“ã‚¸ãƒ§ãƒ³']):
            return self.prompt_templates['planning']
        else:
            return self.prompt_templates['default']
            
    def generate_response_with_confidence(self, query: str, retrieved_docs: List[Dict]) -> Dict:
        """ä¿¡é ¼åº¦ä»˜ããƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆ"""
        # æ¤œç´¢æ–‡æ›¸ã®é–¢é€£åº¦è©•ä¾¡
        relevance_scores = [doc.get('relevance_score', 0.5) for doc in retrieved_docs]
        avg_relevance = sum(relevance_scores) / len(relevance_scores) if relevance_scores else 0
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰
        context = self._build_enhanced_context(retrieved_docs)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé¸æŠ
        prompt_template = self.select_prompt_template(query)
        prompt = prompt_template.format(context=context, query=query)
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆï¼ˆå®Ÿè£…ã¯ä½¿ç”¨ã™ã‚‹LLMã«ã‚ˆã‚‹ï¼‰
        response = self._generate_llm_response(prompt)
        
        return {
            'answer': response,
            'confidence_score': avg_relevance,
            'source_documents': [doc.get('source', 'Unknown') for doc in retrieved_docs],
            'context_quality': self._assess_context_quality(context)
        }
        
    def _build_enhanced_context(self, docs: List[Dict]) -> str:
        """å¼·åŒ–ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ§‹ç¯‰"""
        context_parts = []
        
        for i, doc in enumerate(docs, 1):
            source = doc.get('source', 'Unknown')
            content = doc.get('content', '')
            
            context_part = f"ã€è³‡æ–™{i}ã€‘{source}\n{content}\n"
            context_parts.append(context_part)
            
        return "\n".join(context_parts)
```

---

## Phase 4: è©•ä¾¡ãƒ»ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ 

### 4.1 ç²¾åº¦è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ 

#### **ãƒ•ã‚¡ã‚¤ãƒ«**: `evaluation_system.py` (æ–°è¦ä½œæˆ)

ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–°è¦ä½œæˆã™ã‚‹ã“ã¨ã§ã€ä»¥ä¸‹ã®åŠ¹æœãŒæœŸå¾…ã§ãã¾ã™ã€‚
- RAGã‚·ã‚¹ãƒ†ãƒ ã®æ€§èƒ½ã‚’å®šé‡çš„ã«æ¸¬å®šãƒ»ç›£è¦–
- æ”¹å–„æ–½ç­–ã®åŠ¹æœã‚’å®¢è¦³çš„ã«è©•ä¾¡
- ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®æ€§èƒ½åˆ†æã«ã‚ˆã‚‹å¼±ç‚¹ã®ç‰¹å®š
- ç¶™ç¶šçš„æ”¹å–„ã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ–ãƒ³ãªæ„æ€æ±ºå®šæ”¯æ´

```python
class RAGEvaluationSystem:
    """RAGã‚·ã‚¹ãƒ†ãƒ ã®ç²¾åº¦è©•ä¾¡"""
    
    def __init__(self):
        self.test_queries = [
            {
                'query': 'ç¦å±±å¸‚ã®è¦³å…‰ã‚¹ãƒãƒƒãƒˆã‚’æ•™ãˆã¦',
                'expected_sources': ['fukuyama_tourism_pamphlet_etto.pdf'],
                'category': 'tourism'
            },
            {
                'query': 'ç¦å±±å¸‚ã®å°†æ¥è¨ˆç”»ã«ã¤ã„ã¦',
                'expected_sources': ['fukuyama_comprehensive_plan_basic_concept.pdf'],
                'category': 'planning'
            }
            # è¿½åŠ ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹...
        ]
        
    def evaluate_retrieval_accuracy(self, rag_system) -> Dict:
        """æ¤œç´¢ç²¾åº¦ã®è©•ä¾¡"""
        results = {
            'total_queries': len(self.test_queries),
            'correct_retrievals': 0,
            'precision_scores': [],
            'recall_scores': [],
            'category_performance': {}
        }
        
        for test_case in self.test_queries:
            retrieved_docs = rag_system.retrieve_documents(test_case['query'])
            retrieved_sources = [doc.get('source', '') for doc in retrieved_docs]
            
            # ç²¾åº¦è¨ˆç®—
            precision = self._calculate_precision(
                retrieved_sources, 
                test_case['expected_sources']
            )
            recall = self._calculate_recall(
                retrieved_sources,
                test_case['expected_sources'] 
            )
            
            results['precision_scores'].append(precision)
            results['recall_scores'].append(recall)
            
            # ã‚«ãƒ†ã‚´ãƒªåˆ¥æ€§èƒ½è¨˜éŒ²
            category = test_case['category']
            if category not in results['category_performance']:
                results['category_performance'][category] = []
            results['category_performance'][category].append({
                'precision': precision,
                'recall': recall
            })
            
        # å¹³å‡ç²¾åº¦è¨ˆç®—
        results['avg_precision'] = sum(results['precision_scores']) / len(results['precision_scores'])
        results['avg_recall'] = sum(results['recall_scores']) / len(results['recall_scores'])
        results['f1_score'] = 2 * (results['avg_precision'] * results['avg_recall']) / (results['avg_precision'] + results['avg_recall'])
        
        return results
        
    def generate_evaluation_report(self, results: Dict) -> str:
        """è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        report = f"""
# RAGã‚·ã‚¹ãƒ†ãƒ ç²¾åº¦è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ

## å…¨ä½“æ€§èƒ½
- **å¹³å‡ç²¾åº¦ (Precision)**: {results['avg_precision']:.3f}
- **å¹³å‡å†ç¾ç‡ (Recall)**: {results['avg_recall']:.3f}  
- **F1ã‚¹ã‚³ã‚¢**: {results['f1_score']:.3f}

## ã‚«ãƒ†ã‚´ãƒªåˆ¥æ€§èƒ½
"""
        for category, performances in results['category_performance'].items():
            avg_precision = sum(p['precision'] for p in performances) / len(performances)
            avg_recall = sum(p['recall'] for p in performances) / len(performances)
            
            report += f"""
### {category.title()}
- ç²¾åº¦: {avg_precision:.3f}
- å†ç¾ç‡: {avg_recall:.3f}
"""
        
        return report
```

---

## ğŸ”§ **å®Ÿè£…ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³**

### ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ã®æ®µéšçš„å°å…¥

#### **Step 1**: åŸºæœ¬æ”¹å–„ã®é©ç”¨
```bash
# æ–°ã—ã„å‡¦ç†å™¨ã§ã®å®Ÿè¡Œ
python pdf_processor_enhanced.py

# ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢å†æ§‹ç¯‰
python scripts/rebuild_vector_store.py
```

#### **Step 2**: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®æ›´æ–°

æ—¢å­˜ã®ã‚·ãƒ³ãƒ—ãƒ«ã‚¢ãƒ—ãƒªã‚’æ‹¡å¼µã—ã€æ”¹å–„ã•ã‚ŒãŸRAGã‚·ã‚¹ãƒ†ãƒ ã‚’çµ±åˆã—ã¾ã™ã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ã«ã‚ˆã‚Šã€å¾“æ¥ã‚·ã‚¹ãƒ†ãƒ ã¨ã®äº’æ›æ€§ã‚’ä¿ã¡ãªãŒã‚‰æ®µéšçš„ãªç§»è¡Œã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚

```python
# app_simple.pyã®æ›´æ–°ä¾‹
from hybrid_retriever import HybridRetriever
from rag_system_enhanced import EnhancedRAGSystem

class ImprovedSimpleApp:
    def __init__(self):
        # å¾“æ¥ã®ã‚·ã‚¹ãƒ†ãƒ ã¨ã®äº’æ›æ€§ä¿æŒ
        self.basic_rag = RAGSystemSimple()  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨
        self.enhanced_rag = EnhancedRAGSystem()  # æ”¹å–„ç‰ˆ
        
    def get_response(self, query: str, use_enhanced: bool = True):
        if use_enhanced:
            try:
                return self.enhanced_rag.generate_response_with_confidence(query)
            except Exception as e:
                print(f"Enhanced system error: {e}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                return self.basic_rag.get_response(query)
        else:
            return self.basic_rag.get_response(query)
```

#### **Step 3**: æ€§èƒ½è©•ä¾¡ã®å®Ÿè¡Œ

æ”¹å–„ã•ã‚ŒãŸRAGã‚·ã‚¹ãƒ†ãƒ ã®æ€§èƒ½ã‚’å®šé‡çš„ã«æ¸¬å®šã—ã€æ”¹å–„åŠ¹æœã‚’æ¤œè¨¼ã—ã¾ã™ã€‚ã“ã®è©•ä¾¡ã«ã‚ˆã‚Šã€ã•ã‚‰ãªã‚‹æ”¹å–„ã®æ–¹å‘æ€§ã‚’æ±ºå®šã§ãã¾ã™ã€‚

```python
# æ”¹å–„åŠ¹æœã®æ¸¬å®š
evaluator = RAGEvaluationSystem()
results = evaluator.evaluate_retrieval_accuracy(enhanced_rag_system)
report = evaluator.generate_evaluation_report(results)
print(report)
```

---

## ğŸ“ˆ **æœŸå¾…ã•ã‚Œã‚‹æ”¹å–„åŠ¹æœ**

### å®šé‡çš„ç›®æ¨™ï¼ˆä»®ï¼‰

| æŒ‡æ¨™ | ç¾åœ¨ (æ¨å®š) | Phase 1å¾Œ | Phase 2å¾Œ | Phase 3å¾Œ |
|------|-------------|------------|------------|------------|
| æ¤œç´¢ç²¾åº¦ (Precision) | 0.6 | 0.75 | 0.85 | 0.90 |
| æ¤œç´¢å†ç¾ç‡ (Recall) | 0.5 | 0.65 | 0.80 | 0.85 |
| å¿œç­”å“è³ª (ä¸»è¦³) | æ™®é€š | è‰¯å¥½ | å„ªç§€ | éå¸¸ã«å„ªç§€ |
| å‡¦ç†æ™‚é–“ | åŸºæº– | +20% | +40% | +60% |

### å®šæ€§çš„æ”¹å–„

- **Phase 1**: ã‚ˆã‚Šå¤šãã®æ–‡æ›¸å†…å®¹ã®æ´»ç”¨ã€æ–‡è„ˆä¿æŒã®å‘ä¸Š
- **Phase 2**: é–¢é€£æ€§ã®é«˜ã„æ–‡æ›¸ã®ç™ºè¦‹ã€æ¤œç´¢æ¼ã‚Œã®å‰Šæ¸›  
- **Phase 3**: è‡ªç„¶ã§çš„ç¢ºãªå›ç­”ã€ä¿¡é ¼æ€§ã®å‘ä¸Š
- **Phase 4**: ç¶™ç¶šçš„æ”¹å–„ã®ãŸã‚ã®è©•ä¾¡åŸºç›¤

---

## ğŸ› ï¸ **é–‹ç™ºæ¨å¥¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆä»®ï¼‰**

### çŸ­æœŸ (1-2é€±é–“)
- [ ] `pdf_processor_enhanced.py` ã®å®Ÿè£…
- [ ] ãƒãƒ£ãƒ³ã‚¯æˆ¦ç•¥ã®æœ€é©åŒ–
- [ ] åŸºæœ¬çš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ”¹å–„

### ä¸­æœŸ (1-2ãƒ¶æœˆ)  
- [ ] ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã®å°å…¥
- [ ] ã‚¯ã‚¨ãƒªæ‹¡å¼µæ©Ÿèƒ½ã®å®Ÿè£…
- [ ] è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰

### é•·æœŸ (3-6ãƒ¶æœˆ)
- [ ] é«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«ã®å°å…¥æ¤œè¨
- [ ] ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®Ÿæ–½
- [ ] ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç’°å¢ƒã¸ã®æœ€é©åŒ–

---

## ğŸ’¡ **çµè«–**

æœ¬ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§æç¤ºã—ã¦ã„ã‚‹æ”¹å–„æ¡ˆã¯ã€ç¾åœ¨ã®ãƒ‡ãƒ¢ç‰ˆRAGã‚·ã‚¹ãƒ†ãƒ ã‚’æ®µéšçš„ã«é«˜åº¦åŒ–ã™ã‚‹ãŸã‚ã®ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—ã§ã™ã€‚

### é–‹ç™ºæ–¹é‡
1. **æ®µéšçš„æ”¹å–„**: ãƒªã‚¹ã‚¯ã‚’æœ€å°åŒ–ã—ãªãŒã‚‰ç¢ºå®Ÿã«ç²¾åº¦å‘ä¸Š
2. **äº’æ›æ€§ä¿æŒ**: æ—¢å­˜æ©Ÿèƒ½ã‚’ç¶­æŒã—ãªãŒã‚‰æ–°æ©Ÿèƒ½ã‚’è¿½åŠ 
3. **è©•ä¾¡é§†å‹•**: å®šé‡çš„æŒ‡æ¨™ã«ã‚ˆã‚‹ç¶™ç¶šçš„æ”¹å–„

### ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³å±•é–‹ã¸ã®é“ç­‹
ãƒ‡ãƒ¢ç‰ˆã‹ã‚‰æœ¬æ ¼é‹ç”¨ã‚·ã‚¹ãƒ†ãƒ ã¸ã®ç™ºå±•ã«ãŠã„ã¦ã€æœ¬æ”¹å–„æ¡ˆã¯æŠ€è¡“çš„å®Ÿç¾å¯èƒ½æ€§ã¨è²»ç”¨å¯¾åŠ¹æœã‚’è€ƒæ…®ã—ãŸå®Ÿè·µçš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã ã¨è€ƒãˆã¦ã„ã¾ã™ã€‚

---

**æ–‡æ›¸ä½œæˆæ—¥**: 2025å¹´6æœˆ13æ—¥  
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0  
**å¯¾è±¡ã‚·ã‚¹ãƒ†ãƒ **: rag-chatbot-demo v1.0 (ãƒ‡ãƒ¢ç‰ˆ)
